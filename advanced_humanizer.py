#!/usr/bin/env python3
"""
Advanced Text Humanizer - Aggressive AI Detection Bypass
Reduces AI detection scores to under 10%
"""

import re
import random
import nltk
from collections import defaultdict
import string

# Global NLTK data download function
def ensure_nltk_data():
    """Ensure all required NLTK data is downloaded"""
    required_downloads = [
        ('tokenizers/punkt', 'punkt'),
        ('corpora/wordnet', 'wordnet'),
        ('corpora/omw-1.4', 'omw-1.4')
    ]
    
    for data_path, download_name in required_downloads:
        try:
            nltk.data.find(data_path)
        except LookupError:
            print(f"Downloading {download_name}...")
            nltk.download(download_name, quiet=True)

# Ensure NLTK data is available
ensure_nltk_data()

class AdvancedHumanizer:
    def __init__(self):
        # Ensure NLTK data is available when class is initialized
        ensure_nltk_data()
            
        # Aggressive word replacements - AI to Human
        self.aggressive_replacements = {
            # Formal academic words â†’ Casual equivalents
            "utilize": "use", "utilization": "use", "utilized": "used",
            "implement": "do", "implementation": "doing", "implemented": "did",
            "facilitate": "help", "facilitating": "helping", "facilitated": "helped",
            "demonstrate": "show", "demonstrates": "shows", "demonstrated": "showed",
            "indicate": "show", "indicates": "shows", "indicated": "showed",
            "significant": "big", "significantly": "really", 
            "substantial": "large", "substantially": "really",
            "numerous": "many", "various": "different", "multiple": "many",
            "however": "but", "nevertheless": "but", "furthermore": "also",
            "therefore": "so", "consequently": "so", "subsequently": "then",
            "moreover": "also", "additionally": "also", "alternatively": "or",
            "approximately": "about", "approximately": "around",
            "methodology": "method", "methodologies": "methods",
            "optimization": "improvement", "optimizations": "improvements",
            "enhancement": "improvement", "enhancements": "improvements",
            "acquisition": "getting", "acquire": "get", "acquired": "got",
            "comprehension": "understanding", "comprehend": "understand",
            "determination": "finding out", "determine": "find out",
            "examination": "looking at", "examine": "look at",
            "investigation": "checking", "investigate": "check",
            "establishment": "setting up", "establish": "set up",
            "consideration": "thinking about", "consider": "think about",
            "evaluation": "checking", "evaluate": "check",
            "analysis": "breakdown", "analyze": "break down",
            "synthesis": "putting together", "synthesize": "put together",
            "verification": "checking", "verify": "check",
            "modification": "change", "modify": "change", "modified": "changed",
            "generation": "making", "generate": "make", "generated": "made",
            "creation": "making", "create": "make", "created": "made",
            "construction": "building", "construct": "build", "constructed": "built",
            "development": "building", "develop": "build", "developed": "built",
            "production": "making", "produce": "make", "produced": "made",
            "administration": "running", "administer": "run", "administered": "ran",
            "coordination": "organizing", "coordinate": "organize", "coordinated": "organized",
            "collaboration": "working together", "collaborate": "work together",
            "communication": "talking", "communicate": "talk", "communicated": "talked",
            "documentation": "writing down", "document": "write down", "documented": "wrote down",
            "specification": "details", "specify": "detail", "specified": "detailed",
            "requirement": "need", "requirements": "needs", "required": "needed",
            "recommendation": "suggestion", "recommend": "suggest", "recommended": "suggested",
            "conclusion": "ending", "conclude": "end", "concluded": "ended",
            "decision": "choice", "decide": "choose", "decided": "chose",
            "selection": "picking", "select": "pick", "selected": "picked",
            "identification": "finding", "identify": "find", "identified": "found",
            "recognition": "spotting", "recognize": "spot", "recognized": "spotted",
            "observation": "seeing", "observe": "see", "observed": "saw",
            "notification": "telling", "notify": "tell", "notified": "told",
            "information": "info", "informational": "info-based",
            "operational": "working", "operations": "work", "operate": "work",
            "functional": "working", "function": "work", "functions": "works",
            "professional": "work-related", "professionalism": "being professional",
            "traditional": "old", "traditionally": "usually",
            "conventional": "normal", "conventionally": "normally",
            "fundamental": "basic", "fundamentally": "basically",
            "essential": "key", "essentially": "basically",
            "critical": "important", "critically": "importantly",
            "optimal": "best", "optimally": "best way",
            "maximum": "most", "maximize": "boost", "maximized": "boosted",
            "minimum": "least", "minimize": "reduce", "minimized": "reduced",
            "superior": "better", "superiority": "being better",
            "inferior": "worse", "inferiority": "being worse",
            "advanced": "newer", "advancement": "improvement",
            "sophisticated": "complex", "sophistication": "complexity",
            "comprehensive": "complete", "comprehensively": "completely",
            "extensive": "wide", "extensively": "widely",
            "intensive": "heavy", "intensively": "heavily",
            "effective": "good", "effectiveness": "how good",
            "efficient": "fast", "efficiency": "speed",
            "accurate": "right", "accuracy": "being right",
            "precise": "exact", "precision": "being exact",
            "reliable": "dependable", "reliability": "dependability",
            "consistent": "steady", "consistency": "steadiness",
            "persistent": "lasting", "persistence": "lasting",
            "continuous": "ongoing", "continuously": "ongoing",
            "simultaneous": "at the same time", "simultaneously": "at the same time",
            "immediate": "instant", "immediately": "right away",
            "subsequent": "next", "subsequently": "then",
            "previous": "earlier", "previously": "before",
            "initial": "first", "initially": "at first",
            "final": "last", "finally": "in the end",
            "ultimate": "final", "ultimately": "in the end",
            "primary": "main", "primarily": "mainly",
            "secondary": "second", "secondarily": "secondly",
            "tertiary": "third",
            "alternative": "other", "alternatively": "or",
            "additional": "extra", "additionally": "also",
            "supplementary": "extra", "supplement": "add to",
            "complementary": "matching", "complement": "match",
            "proportional": "matching", "proportion": "part",
            "equivalent": "equal", "equivalence": "equality",
            "identical": "same", "identity": "sameness",
            "similar": "alike", "similarity": "likeness",
            "different": "unlike", "difference": "gap",
            "distinct": "separate", "distinction": "separation",
            "unique": "one-of-a-kind", "uniqueness": "being one-of-a-kind",
            "specific": "exact", "specifically": "exactly",
            "general": "broad", "generally": "broadly",
            "particular": "specific", "particularly": "especially",
            "individual": "single", "individually": "one by one",
            "collective": "group", "collectively": "as a group",
            "universal": "worldwide", "universally": "worldwide",
            "global": "worldwide", "globally": "worldwide",
            "local": "nearby", "locally": "nearby",
            "regional": "area-based", "regionally": "by area",
            "national": "country-wide", "nationally": "country-wide",
            "international": "between countries", "internationally": "between countries"
        }
        
        # Sentence starters that sound human
        self.human_starters = [
            "Well, ", "Actually, ", "You know, ", "Honestly, ", "Look, ",
            "Listen, ", "So, ", "Anyway, ", "I mean, ", "To be fair, ",
            "Let's be real, ", "Here's the thing - ", "The way I see it, ",
            "From what I can tell, ", "As far as I know, ", "It seems like ",
            "Basically, ", "Pretty much, ", "More or less, ", "Kind of ",
            "Sort of ", "I guess ", "Maybe ", "Probably ", "Likely ",
            "It's like ", "Think about it - ", "Consider this: ",
            "Get this - ", "Check it out - ", "Here's what happens: "
        ]
        
        # Conversational connectors
        self.connectors = [
            "and then", "so then", "after that", "next thing", "plus",
            "on top of that", "what's more", "not to mention", "besides",
            "by the way", "speaking of which", "while we're at it",
            "come to think of it", "now that I think about it"
        ]
        
        # Filler words and expressions
        self.fillers = [
            "you know", "like", "I mean", "sort of", "kind of", "pretty much",
            "more or less", "or something", "or whatever", "and stuff",
            "and things like that", "and all that", "you get the idea",
            "if you know what I mean", "right?", "you see"
        ]
        
        # Contractions for natural speech
        self.contractions = {
            "do not": "don't", "does not": "doesn't", "did not": "didn't",
            "will not": "won't", "would not": "wouldn't", "could not": "couldn't",
            "should not": "shouldn't", "cannot": "can't", "must not": "mustn't",
            "have not": "haven't", "has not": "hasn't", "had not": "hadn't",
            "is not": "isn't", "are not": "aren't", "was not": "wasn't",
            "were not": "weren't", "it is": "it's", "that is": "that's",
            "there is": "there's", "here is": "here's", "what is": "what's",
            "where is": "where's", "when is": "when's", "how is": "how's",
            "who is": "who's", "I am": "I'm", "you are": "you're",
            "we are": "we're", "they are": "they're", "I will": "I'll",
            "you will": "you'll", "we will": "we'll", "they will": "they'll",
            "I would": "I'd", "you would": "you'd", "we would": "we'd",
            "they would": "they'd", "I have": "I've", "you have": "you've",
            "we have": "we've", "they have": "they've"
        }
        
        # AI-specific phrase patterns to break
        self.ai_patterns = [
            r'\bin conclusion\b', r'\bin summary\b', r'\bto summarize\b',
            r'\bin essence\b', r'\bfurthermore\b', r'\bmoreover\b',
            r'\badditionally\b', r'\bnevertheless\b', r'\bhowever\b',
            r'\bconsequently\b', r'\btherefore\b', r'\bthus\b',
            r'\bhence\b', r'\baccordingly\b', r'\bsubsequently\b'
        ]
        
    def aggressive_humanize(self, text):
        """Apply aggressive humanization techniques"""
        
        # Step 1: Replace formal words with casual ones
        for formal, casual in self.aggressive_replacements.items():
            # Word boundary replacement
            pattern = r'\b' + re.escape(formal) + r'\b'
            text = re.sub(pattern, casual, text, flags=re.IGNORECASE)
        
        # Step 2: Break up AI sentence patterns
        sentences = nltk.sent_tokenize(text)
        humanized_sentences = []
        
        for i, sentence in enumerate(sentences):
            # Remove AI transition words at sentence starts
            for pattern in self.ai_patterns:
                sentence = re.sub(pattern + r'\s*,?\s*', '', sentence, flags=re.IGNORECASE)
            
            # Add human-like sentence starters (30% chance)
            if random.random() < 0.3 and len(sentence.split()) > 3:
                starter = random.choice(self.human_starters)
                sentence = starter + sentence.lower()
            
            # Add fillers within sentences (20% chance)
            if random.random() < 0.2 and len(sentence.split()) > 5:
                words = sentence.split()
                insert_pos = random.randint(2, len(words) - 2)
                filler = random.choice(self.fillers)
                words.insert(insert_pos, filler + ",")
                sentence = " ".join(words)
            
            # Replace connectors with casual ones
            if i > 0 and random.random() < 0.4:
                sentence = random.choice(self.connectors) + ", " + sentence.lower()
            
            humanized_sentences.append(sentence)
        
        text = " ".join(humanized_sentences)
        
        # Step 3: Apply contractions aggressively
        for formal, contraction in self.contractions.items():
            text = re.sub(r'\b' + re.escape(formal) + r'\b', contraction, text, flags=re.IGNORECASE)
        
        # Step 4: Add casual punctuation and expressions
        text = self._add_casual_elements(text)
        
        # Step 5: Vary sentence structure
        text = self._vary_sentence_structure(text)
        
        # Step 6: Add human imperfections
        text = self._add_human_imperfections(text)
        
        return text.strip()
    
    def _add_casual_elements(self, text):
        """Add casual punctuation and expressions"""
        
        # Replace some periods with ellipses (10% chance)
        sentences = text.split('. ')
        for i in range(len(sentences)):
            if random.random() < 0.1:
                sentences[i] = sentences[i] + "..."
            elif random.random() < 0.05:
                sentences[i] = sentences[i] + "!"
        
        text = '. '.join(sentences)
        
        # Add casual expressions
        casual_additions = [
            ", right?", ", you know?", ", if you ask me", ", honestly",
            ", to be honest", ", let's be real", ", no joke", ", seriously"
        ]
        
        sentences = text.split('. ')
        for i in range(len(sentences)):
            if random.random() < 0.15:
                addition = random.choice(casual_additions)
                sentences[i] = sentences[i] + addition
        
        return '. '.join(sentences)
    
    def _vary_sentence_structure(self, text):
        """Vary sentence structure to avoid AI patterns"""
        
        sentences = nltk.sent_tokenize(text)
        varied_sentences = []
        
        for sentence in sentences:
            words = sentence.split()
            
            # Occasionally start with different structures
            if len(words) > 4 and random.random() < 0.2:
                # Move some elements around
                if words[0].lower() in ['the', 'this', 'that', 'these', 'those']:
                    # Try to restructure
                    if 'is' in words or 'are' in words or 'was' in words or 'were' in words:
                        # Find the verb and potentially restructure
                        pass  # Keep original for now, could add more complex restructuring
            
            varied_sentences.append(sentence)
        
        return ' '.join(varied_sentences)
    
    def _add_human_imperfections(self, text):
        """Add subtle human-like imperfections"""
        
        # Occasionally use less formal grammar
        replacements = [
            (r'\bwho are\b', 'that are'),
            (r'\bwhich are\b', 'that are'),
            (r'\bamong\b', 'between'),
            (r'\bregarding\b', 'about'),
            (r'\bconcerning\b', 'about'),
            (r'\bprior to\b', 'before'),
            (r'\bsubsequent to\b', 'after'),
            (r'\bin order to\b', 'to'),
            (r'\bdue to the fact that\b', 'because'),
            (r'\bfor the reason that\b', 'because'),
            (r'\bin spite of the fact that\b', 'even though'),
            (r'\bnotwithstanding the fact that\b', 'even though')
        ]
        
        for pattern, replacement in replacements:
            if random.random() < 0.3:  # 30% chance to apply each
                text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def humanize(self, text):
        """Main humanization method"""
        if not text or not text.strip():
            return text
        
        # Apply aggressive humanization
        result = self.aggressive_humanize(text)
        
        # Ensure first letter is capitalized
        if result:
            result = result[0].upper() + result[1:] if len(result) > 1 else result.upper()
        
        return result
